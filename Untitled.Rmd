---
title: "Predicting and Characterizing Stocks"
output: 
  flexdashboard::flex_dashboard:
    orientation: columns
    vertical_layout: fill
---

```{r setup, include=FALSE}
library(flexdashboard)
require(ggplot2)
require(plgp)
require(mvtnorm)
require(weather)
require(MASS)
require(grid)
require(gridExtra)
require(ggtext)
require(tidyr)
require(corrplot)
require(gganimate)
require(readr)
require(magick)

source("utils.R")
ps = read.csv("public_storage_dat.csv")
nvda = read.csv("nvidia_dat.csv")

acc = read.csv("error.csv")
uncertainty = read.csv("GP_uncertainty.csv")

all_stocks = read.csv("all_stock_data33.csv")

env5 = read.csv("env_5choices.csv")
env5_post = read.csv("env_5choices_posterior.csv")
env5_var = read.csv("env_5choices_var.csv")
env5_probs = read.csv("env_5choices_probs.csv")

env25 = read.csv("env_25choices.csv")
env25_post = read.csv("env_25choices_posterior.csv")
env25_var = read.csv("env_25choices_var.csv")
env25_probs = read.csv("env_25choices_probs.csv")

env5_probs$value = env5_probs$value
env25_probs$value = env25_probs$value

load("exploration_bonus.RData")

cors = read.csv("volume_value_corr_matrix.csv")

load("exploration_bonus_large.RData")

cors_large = read.csv("volume_value_corr_matrix_large.csv")

load("lstm_bitcoin_preds.RData")
```
Stock Performance {data-navmenu="Paradigm Shift in Stocks"}
=======================================

Column {data-width=600}
-----------------------------------------------------------------------

### Change in leading stocks

```{r}
gganimate::gif_file("info_ts.gif")
```

Column {data-width=400}
-----------------------------------------------------------------------

### Changes in stocks over time

```{r}
dat = all_stocks[all_stocks$x %in% c(1,1259),]
dat$x = factor(dat$x)
mycolors = rep("gray",nrow(dat))
mycolors[dat$type=="information tech"] = "orange"
mycolors[dat$type=="real estate"] = "blue"
names(mycolors) = dat$type
ggplot(dat,aes(x=x,y=close,col=type,group=Name,fill=type))+
  geom_line(size=2)+
  geom_point(shape=21,size=4,col="black")+
  scale_color_manual(values = mycolors)+
  scale_fill_manual(values = mycolors)+
  labs(y="market value",x="",
       title="**Changes in market value<br>from 2012 to 2018<br>for <span style='color:orange'>Information Technology</span> <br>and <span style='color:blue'>Real Estate</span> Stocks**")+
  scale_x_discrete(labels = c("Feb 2012","Feb 2018"))+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black"),
        text = element_text(size=20),axis.ticks.x = element_blank(),
        legend.position = "None",
        plot.title = element_markdown())+
  annotate("text",x = 2.2,y = dat$close[dat$x==1259&dat$Name=="NVDA"],
           label="NVIDIA",size=5,col="orange",fontface="bold")
```

Integration of IT {data-navmenu="Paradigm Shift in Stocks"}
=======================================

Column {data-width=600}
-----------------------------------------------------------------------

### Stock types with 2 clusters

```{r}
data_wide = pivot_wider(all_stocks,names_from = type,values_from = close)[,9:19]
data_wide = apply(data_wide,2,na.omit)
corrplot(cor(data_wide),methods="color",addrect=2,order="hclust")
```


Column {data-width=600}
-----------------------------------------------------------------------

### Stock types with 3 clusters

```{r}
data_wide = pivot_wider(all_stocks,names_from = type,values_from = close)[,9:19]
data_wide = apply(data_wide,2,na.omit)
corrplot(cor(data_wide),methods="color",addrect=3,order="hclust")
```

Larger sample size {data-navmenu="Paradigm Shift in Stocks"}
=======================================

Column {data-width=600}
-----------------------------------------------------------------------

### Sell Real Estate, Buy Info Tech

```{r}
gganimate::gif_file("trades.gif")
```


Gaussian Processes {data-navmenu="Predicting Stocks"}
=======================================

Column {.sidebar}
-----------------------------------------------------------------------

To understand the internal mechanics underlying a Gaussian Process, I created this example page. Say we have 10 days where we recorded the weather, which ranges from rainy (bottom of y-axis) to sunny (top of y-axis). We want to predict what the weather will be like on the eleventh day. A Gaussian Process first computes the similarity of yesterday's weather from to the other nine days' weather. In this simplified case, similarity would be the closeness of each day to yesterday on the y-axis. We see that day 1 has the most similar weather to yesterday's weather. Assuming the weather follows a pattern, a Gaussian Process next calculates the slope of the line connecting the most similar day (day 1) to the next day (day 2). Assuming a pattern, the Gaussian Process assumes that the slope between yesterday and today's (predicted) weather will be similar to the slope between day 1 and 2. Thus, the model then applies this slope to yesterday, and the predicted weather is simply where the vectors tracking the slope point to. 

Column {data-width=600}
-----------------------------------------------------------------------
### How GP makes predictions
```{r}
inputs = seq(from=0,to=2*pi,length=10)
yy = sin(inputs)
df = data.frame(x = inputs,y=yy,
                weather = c("day-rain-mix","day-rain-mix",
                "day-sunny","day-sunny","day-rain-mix","day-rain-mix",
                "rain","rain","day-rain-mix","day-rain-mix"))
p1 = ggplot(df)+
  geom_point(aes(x,y))+
  geom_line(aes(x,y))+
  geom_weather(aes(x,y,weather=weather))+
  theme_void()+
  labs(x = "days",y = "sunny weather")+
  scale_x_continuous(limits = c(0,8.5))+
  annotate("text",x=6,y=0.7,label="What will the \nweather be \non this day?",fontface="bold",size=7)+
  scale_y_continuous(breaks=0,labels="")+
  geom_vline(xintercept = 7.5,col="red",size=1.5,linetype="dotted",alpha=.5)
p2 = ggplot(df)+
  geom_point(aes(x,y))+
  geom_line(aes(x,y))+
  geom_weather(aes(x,y,weather=weather))+
  theme_void()+
  labs(x = "days",y = "sunny weather")+
  scale_x_continuous(limits = c(0,8.5))+
  annotate("text",x=7.5,y=0.7,label="",fontface="bold")+
  scale_y_continuous(breaks=0,labels="")+
  geom_vline(xintercept = 7.5,col="red",size=1.5,linetype="dotted",alpha=.1)+
  annotate("segment",x=df$x[10],xend=df$x[7],y=df$y[10],yend=df$y[7],arrow = arrow(),col="gray",size=1.5)+
  annotate("segment",x=df$x[10],xend=df$x[6],y=df$y[10],yend=df$y[6],arrow = arrow(),col="gray",size=1.5)+
  annotate("segment",x=df$x[10],xend=df$x[5],y=df$y[10],yend=df$y[5],arrow = arrow(),col="gray",size=1.5)+
  annotate("segment",x=df$x[10],xend=df$x[4],y=df$y[10],yend=df$y[4],arrow = arrow(),col="gray",size=1.5)+
  annotate("text",x = df$x[10],y = 0.7,label = "First, compute similarity of \nprevious day's weather \nto all other days",size=6)

p3 = ggplot(df)+
  geom_point(x=df$x[1],y=df$y[1],shape=21,fill="gray",col="red",size=15)+
  geom_point(aes(x,y))+
  geom_line(aes(x,y))+
  geom_weather(aes(x,y,weather=weather))+
  theme_void()+
  labs(x = "days",y = "sunny weather")+
  scale_x_continuous(limits = c(0,8.5))+
  annotate("text",x=7.5,y=0.7,label="",fontface="bold")+
  scale_y_continuous(breaks=0,labels="")+
  geom_vline(xintercept = 7.5,col="red",size=1.5,linetype="dotted",alpha=.1)+
  annotate("segment",x=df$x[10],xend=df$x[1],y=df$y[10],yend=df$y[1],arrow = arrow(),col="red",size=1.5)+
  annotate("text",x = df$x[3],y = -0.3,label = "The most similar day",size=6)

p4 = ggplot(df)+
  geom_point(aes(x,y))+
  geom_line(aes(x,y))+
  geom_weather(aes(x,y,weather=weather))+
  theme_void()+
  labs(x = "days",y = "sunny weather")+
  scale_x_continuous(limits = c(0,8.5))+
  annotate("text",x=7.5,y=0.7,label="",fontface="bold")+
  scale_y_continuous(breaks=0,labels="")+
  geom_vline(xintercept = 7.5,col="red",size=1.5,linetype="dotted",alpha=.1)+
  annotate("segment",x=0,xend=0,y=0,yend=0.4,arrow = arrow(),col="blue",size=2)+
  annotate("segment",x=0,xend=df$x[2],y=0.4,yend=df$y[2],arrow = arrow(),col="darkred",size=2)+
  annotate("text",x = df$x[10],y = 0.7,label = "Next, compute slope \nfrom most similar day \nto its following day",size=6)

p5 = ggplot(df)+
  geom_point(aes(x,y))+
  geom_line(aes(x,y))+
  geom_weather(aes(x,y,weather=weather))+
  theme_void()+
  labs(x = "days",y = "sunny weather")+
  scale_x_continuous(limits = c(0,8.5))+
  annotate("text",x=7.5,y=0.7,label="",fontface="bold")+
  scale_y_continuous(breaks=0,labels="")+
  geom_vline(xintercept = 7.5,col="red",size=1.5,linetype="dotted",alpha=.1)+
  annotate("segment",x=0,xend=0,y=0,yend=0.4,arrow = arrow(),col="blue",size=2,alpha=0.5)+
  annotate("segment",x=0,xend=df$x[2],y=0.4,yend=df$y[2],arrow = arrow(),col="darkred",size=2,alpha=0.5)+
  annotate("segment",x=df$x[10],xend=df$x[10],y=0,yend=0.4,arrow = arrow(),col="blue",size=2)+
  annotate("segment",x=df$x[10],xend=df$x[10] + (df$x[2]-df$x[1]),y=0.4,yend=df$y[2],arrow = arrow(),col="darkred",size=2)+
  annotate("text",x = df$x[7],y = 0.7,label = "Finally, apply \nslope to previous day",size=6)+
  annotate("segment",x=0,xend=df$x[9],y=0.4,yend=0.4,arrow = arrow(),col="gray",size=2,alpha=0.5)

p6 = ggplot(df)+
  geom_point(x = 7.5,y = df$y[2],shape = 21, fill="white",col = "red",size = 20)+
  geom_point(aes(x,y))+
  geom_line(aes(x,y))+
  geom_weather(aes(x,y,weather=weather))+
  theme_void()+
  labs(x = "days",y = "sunny weather")+
  scale_x_continuous(limits = c(0,8.5))+
  annotate("text",x=7.5,y=0.7,label="",fontface="bold")+
  scale_y_continuous(breaks=0,labels="")+
  geom_vline(xintercept = 7.5,col="red",size=1.5,linetype="dotted",alpha=.4)+
  annotate("segment",x=df$x[10],xend=df$x[10],y=0,yend=0.4,arrow = arrow(),col="blue",size=2)+
  annotate("segment",x=df$x[10],xend=df$x[10] + (df$x[2]-df$x[1]),y=0.4,yend=df$y[2],arrow = arrow(),col="darkred",size=2)+
  annotate("text",x = df$x[8],y = 0.7,label = "Predicted Weather",size=6)+
  geom_weather(aes(x = 7.5,y = df$y[2],weather = "day-sunny"))

# animation::saveGIF(
#   expr = {
#     plot(p1)
#     plot(p2)
#     plot(p3)
#     plot(p4)
#     plot(p5)
#     plot(p6)
#   },
#   movie.name = "GP_mechanics.gif"
# )
giff = image_read("GP_mechanics.gif")
image_animate(giff,fps=0.25)
```


Example Gaussian Processes {data-navmenu="Predicting Stocks"}
=======================================

Column {data-width=500}
-----------------------------------------------------------------------

### Just a single sample

```{r}
n_points = 10
n_samples = 1
noise = sqrt(.Machine$double.eps)
X = seq(from=0,to=2*pi,length=n_points)
y = sin(X)+rnorm(10)
input_similarities = exp(-distance(X)) + diag(noise,n_points)
XX = seq(from=-0.5,to=2*pi+1,length=100)
test_similarities = exp(-distance(XX)) + diag(noise,100)
test_input_similarities = exp(-distance(XX,X))
precision_matrix = solve(input_similarities)
mean_vector = test_input_similarities %*% precision_matrix %*% y
covariance_matrix = test_similarities - test_input_similarities %*%precision_matrix %*% t(test_input_similarities)
function_samples = rmvnorm(n_samples,mean_vector,covariance_matrix)
lower_bound = mean_vector + qnorm(0.05,0,sqrt(diag(covariance_matrix)))
upper_bound = mean_vector + qnorm(0.95,0,sqrt(diag(covariance_matrix)))
matplot(XX,t(function_samples),type="l",col="gray",lty=1,xlab="input",ylab="output",main="Gaussian Process with 1 sample")
points(X,y,pch=20,cex=2)
lines(XX,mean_vector,lwd=2)
lines(XX,lower_bound,lwd=2,lty=2,col=2)
lines(XX,upper_bound,lwd=2,lty=2,col=2)
```


### Ten samples

```{r}
n_samples = 10
function_samples = rmvnorm(n_samples,mean_vector,covariance_matrix)
lower_bound = mean_vector + qnorm(0.05,0,sqrt(diag(covariance_matrix)))
upper_bound = mean_vector + qnorm(0.95,0,sqrt(diag(covariance_matrix)))
matplot(XX,t(function_samples),type="l",col="gray",lty=1,xlab="input",ylab="output",main="Gaussian Process with 10 samples")
points(X,y,pch=20,cex=2)
lines(XX,mean_vector,lwd=2)
lines(XX,lower_bound,lwd=2,lty=2,col=2)
lines(XX,upper_bound,lwd=2,lty=2,col=2)
```

Column {data-width=500}
-----------------------------------------------------------------------

### One hundred samples

```{r}
n_samples = 100
function_samples = rmvnorm(n_samples,mean_vector,covariance_matrix)
lower_bound = mean_vector + qnorm(0.05,0,sqrt(diag(covariance_matrix)))
upper_bound = mean_vector + qnorm(0.95,0,sqrt(diag(covariance_matrix)))
matplot(XX,t(function_samples),type="l",col="gray",lty=1,xlab="input",ylab="output",main="Gaussian Process with 100 samples")
points(X,y,pch=20,cex=2)
lines(XX,mean_vector,lwd=2)
lines(XX,lower_bound,lwd=2,lty=2,col=2)
lines(XX,upper_bound,lwd=2,lty=2,col=2)
```


### One thousand samples
```{r}
n_samples = 1000
function_samples = rmvnorm(n_samples,mean_vector,covariance_matrix)
lower_bound = mean_vector + qnorm(0.05,0,sqrt(diag(covariance_matrix)))
upper_bound = mean_vector + qnorm(0.95,0,sqrt(diag(covariance_matrix)))
matplot(XX,t(function_samples),type="l",col="gray",lty=1,xlab="input",ylab="output",main="Gaussian Process with 1000 samples")
points(X,y,pch=20,cex=2)
lines(XX,mean_vector,lwd=2)
lines(XX,lower_bound,lwd=2,lty=2,col=2)
lines(XX,upper_bound,lwd=2,lty=2,col=2)
```


Applying GPs to the Stock Market {data-navmenu="Predicting Stocks"}
=======================================

Column {data-width=500}
-----------------------------------------------------------------------

### Comparison of Performance to Linear Model

```{r}
ggplot(acc,aes(x=model,y=error,fill=stock_type))+
  geom_bar(stat="summary",position="dodge",col="black")+
  geom_errorbar(stat="summary",position=position_dodge(0.9),width=0.1)+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black"),
        text = element_text(size=20),axis.ticks.x = element_blank())+
  labs(y="mean squared error",x="")+
  scale_fill_viridis_d()+
  guides(fill=guide_legend(title="Stock Type"))+
  scale_y_continuous(expand=c(0,0))
```

### Predictions for Nvidia

```{r}
plot_gp(train_x = nvda$X[nvda$phase=="train"],
        train_y = nvda$Y[nvda$phase=="train"],
        test_x = nvda$X[nvda$phase=="test"],
        test_y = nvda$Y[nvda$phase=="test"],
        gp_predictions = nvda$predictions_gp[nvda$phase=="test"],
        gp_lower = nvda$lower_bound_gp[nvda$phase=="test"],
        gp_upper = nvda$upper_bound_gp[nvda$phase=="test"],
        lin_predictions = nvda$predictions_lin[nvda$phase=="test"],
        stock = nvda$stock[1],
        stock_type = nvda$stock_type[1])
```

Column {data-width=500}
-----------------------------------------------------------------------
### Predicted Uncertainty

```{r}
ggplot(uncertainty,aes(x=reorder(stock_type,uncertainty),y=uncertainty,fill=stock_type))+
  geom_bar(stat="summary",col="black")+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black"),
        text = element_text(size=20),axis.ticks.x = element_blank(),
        legend.position = "None",axis.text.x = element_text(angle=90))+
  labs(y="normalized \nstandard deviation \n(in US Dollars)",x="",title="Gaussian Process model \nprediction uncertainties")+
  scale_fill_viridis_d()+
  scale_y_continuous(expand=c(0,0))
```

### Public Storage Predictions

```{r}
plot_gp(train_x = ps$X[ps$phase=="train"],
        train_y = ps$Y[ps$phase=="train"],
        test_x = ps$X[ps$phase=="test"],
        test_y = ps$Y[ps$phase=="test"],
        gp_predictions = ps$predictions_gp[ps$phase=="test"],
        gp_lower = ps$lower_bound_gp[ps$phase=="test"],
        gp_upper = ps$upper_bound_gp[ps$phase=="test"],
        lin_predictions = ps$predictions_lin[ps$phase=="test"],
        stock = ps$stock[1],
        stock_type = ps$stock_type[1])
```

To Explore or Exploit {data-orientation=rows data-navmenu="New Stock Typology"}
=======================================

Row {data-height=300}
-----------------------------------------------------------------------

### Background
Possibly ALL decisions can be reduced to choosing to EXPLORE or EXPLOIT. When it is date night, do you return to your favorite restaurant (EXPLOIT) or try a novel location (EXPLORE)? When choosing which career path to embark upon, do you go with something you know for sure will lead to a stable future or do you risk something else? This problem was famously introduced to the public under the name of 'The Secretary Problem', which goes something like the following. A secretary has 100 applicants for a new job opening. She can interview as many as she would like but the interviews will be done one at a time and once she moves on to the next applicant, she cannot call back the previous applicant. Once she chooses an applicant for hire, she cannot interview any more. Is there an optimal number of applicants to interview? For many years, even mathematicians assumed there was no optimal answer, but it turns out there is: 37. With no other knowledge of the options (applicants), the optimal number of samples to draw is 37%. Then you exploit (choose) the next best option. This can be applied to many aspects of decision-making: when searching for a home, assuming no other knowledge besides that there are 100 homes for sale and that they are being sold quickly, you should view 37 of them and then buy the next best one. Some have even popularized this as a dating rule: date for 37% of your expected lifetime and then marry the next best partner. There are interesting order effects that this rule makes clear (explore first, exploit second), but for our purposes, it is important to note that this makes clear that optimal decision-making involves BOTH exploration and exploitation: Without exploration, you will never know if better options exist and without exploitation, you will never reap the benefits.


Row {data-height=300}
-----------------------------------------------------------------------
The exploration-exploitation tradeoff has evolutionary roots. Given that it provides an optimal decision-making strategy, it comes as no surprise that animals adopt such strategies when foraging for food. To simulate an optimal balance of exploration and exploitation using a Gaussian Process model, I have simulated decision-making in an environment. This environment is a simple 8-by-8 grid that can be imagined as a field that has food (resources) scattered throughout. The goal is to obtain as many resources as possible. For any decision-making model, there of course needs to be a learning stage. Thus, the next page shows an example of two environments, one with 5 sampled locations and another with 25 sampled locations. We should expect the model trained on 25 sampled locations to have acquired more useful information than the model that was trained on 5 sampled locations; we will use this assumption as a sanity check once the results are computed. From these initial samples, the model will predict what the value of EVERY location in the environment is. Many previous decision-making models could only make predictions about already-sampled locations, but the Gaussian Process model's covariance function enables it to GENERALIZE acquired knowledge to unsampled options. In brief, it assumes that a spatial location nearby previously sampled location x will have a similar value to x. It computes posterior distributions, as illustrated in the 'Predicting Stocks' tab, enabling calculation of uncertainty, which can be mixed in to the overall computation of predicted values, assuming one is more (add the uncertainty) or less (subtract the uncertainty) exploratory. In other words, an exploitative decision depends entirely on the mean posterior estimate, whereas an exploratory decision will be biased towards posterior estimates with larger widths, ie, more uncertainty.



Initial Choices {data-navmenu="New Stock Typology"}
=======================================

Column {.sidebar}
-----------------------------------------------------------------------

These images show the observed values for the 5 or 25 options that were selected. The colored squares are the locations that were sampled by the hypothetical agent. We will call the agent Bob. So, on the left, Bob searched for food in 5 locations and on the right, Bob searched for food in 25 locations. The numbers in the squares are a representation of how valuable each sampled location was. On the left side, the orange-ish location reads '68', which can mean 68 apples. The darkest green color square on the left yielded 12 apples. Thus, warm colors indicate more resourceful locations and greenish colors indicate locations with few resources.

Column {data-width=500}
-----------------------------------------------------------------------

### 5 Initial Choices from Your Environment

```{r}
plot_choices(env5)
```

Column {data-width=500}
-----------------------------------------------------------------------

### 25 Initial Choices from Your Environment

```{r}
plot_choices(env25)
```


Predicted Values {data-navmenu="New Stock Typology"}
=======================================

Column {.sidebar}
-----------------------------------------------------------------------

These images are the same environments as on the previous page, though this time, the color of each square represents the predicted value of that location by the Gaussian Process model. An important, and relatively unique, aspect of this model's prediction is that it learns functions based on similarities between inputs. For example, on the left side, the color of the squares surrounding the sampled squares are similar to the color of the sampled square. This means that the model assumes that nearby locations will yield similar resources.

Column {data-width=500}
-----------------------------------------------------------------------

### GP Predicted Values After 5

```{r}
plot_posterior(env5_post)
```

Column {data-width=500}
-----------------------------------------------------------------------

### GP Predicted Values After 25

```{r}
plot_posterior(env25_post)
```

Prediction Uncertainty {data-navmenu="New Stock Typology"}
=======================================

Column {.sidebar}
-----------------------------------------------------------------------

The Gaussian Process model is a Bayesian model, which is distinct from many previous models of reward learning. Many previous models of reward learning make point estimates. For example, each square will get a single predicted value. Bayesian models estimate a distribution of values. Thus, the predicted value can be regarded the average value from across a predicted distribution. Given a distribution, however, we can also infer the uncertainty in the model's predicted values because the width of a distribution can be used as a proxy for uncertainty. These plots show the uncertainties for each location in the environment.

Column {data-width=500}
-----------------------------------------------------------------------

### Uncertainty After 5

```{r}
plot_posterior(env5_var)
```

Column {data-width=500}
-----------------------------------------------------------------------

### Uncertainty After 25

```{r}
plot_posterior(env25_var)
```

Next Choice Probabilities {data-navmenu="New Stock Typology"}
=======================================

Column {.sidebar}
-----------------------------------------------------------------------

Now that we have calculated the predicted values and uncertainties, and thus, have measures for how much people rely on exploitation and exploration, respectively, the last thing to do is to transform these predictions into actual decisions. This is formalized as a choice rule. Most researchers use one of two choice rules to emulate biological decision-making: argmax or softmax. Argmax means Bob will choose the option with the highest predicted value every time. Softmax (soft maximization) means Bob will choose the option with the highest predicted value proportional to its probability, or relative predicted value. We use the softmax here and plot the predicted probabilities for every location that Bob might choose next.

Column {data-width=500}
-----------------------------------------------------------------------

### Probabilities of Selecting Each Option After 5

```{r}
plot_probs(env5_probs)
```

Column {data-width=500}
-----------------------------------------------------------------------

### Probabilities of Selecting Each Option After 25

```{r}
plot_probs(env25_probs)
```

Exploration Predicting Stock Value {data-navmenu="New Stock Typology"}
=======================================

Column {.sidebar}
-----------------------------------------------------------------------

How does the exploration-exploitation relate to the stock market? No idea honestly. This is just an idea. All stocks have volatility, which is perhaps more formally known as variance, or variability. The more variability that a stock exhibits, the more uncertain a Gaussian Process will be in its predictions for said stock. Perhaps a useful thing to know is if volatility is simply noise or not. For example, when volatile, does a stock tend to perform better or worse (in which case, its volatility has structure), or is there no pattern to performance under volatility (in which case, its volatility is no different from noise). One measure of a healthy stock is its value-volume relationship: if its market value and trading volume is positively correlated, this indicates that the demand for this stock's supplies is high AND lots of people want to buy. We can thus calculate a new measure of market value (we'll call it 'subjective value' to be consistent with reward learning literature) that is a combination of predicted value and uncertainty, and then correlate subjective value and trading volume. Importantly, here we scaled uncertainty by different values (called an 'exploration bonus', ranging from 0 to 10). If the relationship between value and volume strengthens when reducing uncertainty's influence (exploration bonus < 1), then adding uncertainty into the market value hurts predictability, meaning the stock is better characterized by its mean values. If the relationship between value and volume strengthens when increasing uncertainty's influence (exploration bonus > 1), then adding uncertainty into the market value enhances predictability, meaning the stock is better characterized by both mean value and variance. For the former, we will call those stocks 'exploitative', and for the latter, we will call those stocks 'exploratory'.

Column {data-width=1000}
-----------------------------------------------------------------------

### How Exploration Measures Relate to Trading Volume

```{r}
mylegend = getlegend(all_plots[[5]])
for(i in 1:length(all_plots)){
  all_plots[[i]] = all_plots[[i]] + theme(legend.position="None")
}
grid.arrange(grobs = all_plots[1:20],right = mylegend,
                        bottom = textGrob("trading volume",gp=gpar(fontsize=20)),
                        left= textGrob("subjective value",gp=gpar(fontsize=20),rot=90))
```

Which Stock Types are Exploratory/Exploitative {data-navmenu="New Stock Typology"}
=======================================

Column {data-width=1000}
-----------------------------------------------------------------------

### Exploratory vs Exploitative Stock Types

```{r}
uncertainty_idx = apply(cors,1,function(x) which(x==max(x)))
labeled = as.data.frame(cbind(keep,uncertainty_idx))
labeled$uncertainty_idx = ifelse(labeled$uncertainty_idx==1,"exploit","explore")
labeled$type = type
typologies = as.data.frame(table(labeled$type,labeled$uncertainty_idx))
colnames(typologies)[1] = "stock_type"
typologies$Freq[typologies$Var2=="explore"] = -typologies$Freq[typologies$Var2=="explore"]
ggplot(typologies,aes(x=reorder(stock_type,Freq),y=Freq,fill=stock_type))+
  geom_bar(stat="identity")+
  geom_hline(yintercept = 0)+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black"),
        text = element_text(size=20),legend.position = "None",
        plot.title = element_text(hjust=1))+
  labs(x = "",y = "Exploratory < > Exploitative",
       title="Frequency of stock types that are \nexploratory or exploitative")+
  scale_y_discrete(breaks=c(-2,0,2),labels=c(2,0,2))+
  scale_fill_viridis_d()+
  coord_flip()

```


Is IT really Exploratory? {data-navmenu="New Stock Typology"}
=======================================

Column {.sidebar}
-----------------------------------------------------------------------

The previous page displayed the new typologies for 33 total stocks, 3 from each stock type. I did not analyze all stocks this way because my dataframe downloaded from Kaggle did not come with stock type assignment, so I had to look up each stock name and manually assign it to a stock type. 500 of these would take awhile. However, I did look up an additional 12 information technology stocks to get a better idea of IT's exploratory vs exploitative measure. This is the breakdown of those 15 IT stocks.

Column {data-width=1000}
-----------------------------------------------------------------------

### Majority of IT stocks in S & P 500 are Exploratory

```{r}
uncertainty_idx = apply(cors_large,1,function(x) which(x==max(x)))
labeled = as.data.frame(cbind(each_stock,uncertainty_idx))
labeled$uncertainty_idx = ifelse(labeled$uncertainty_idx==1,"exploit","explore")
type = ifelse(each_stock%in%it_stocks,"information tech","real estate")
labeled$type = type
pie(table(labeled$uncertainty_idx,labeled$type)[,1],col = c("gray","purple"),labels = c("EXPLOIT (6/15)","EXPLORE (9/15)"))
```

Bitcoin Prediction {data-navmenu="BONUS"}
=======================================

Column {.sidebar}
-----------------------------------------------------------------------

Gaussian Processes have limitations as well. It could not predict out-of-sample Bitcoin market values well (data not shown). Here, we tested a more computationally heavy model to see if it could do what the Gaussian Process model could not. We used a long short-term memory neural network model, or LSTM. The ingenuity of LSTMs is that they maintain selective 'memory' of trends from past timepoints. Thus, some information, ideally noise, is learned to be forgotten, while other information is maintained in a short-term memory store. This way, information that occurs periodically but sparsely can still be learned. The distinct colors indicate years (2019 - 2024). The LSTM model was trained on data from 2019 till the end of February 2024, leaving approximately the next two weeks (end of Feb till beginning of March) untouched. The thin colored lines are LSTM model fitted values and predictions for the out-of-sample data. Clearly, it is not perfect (seems to tend to underestimate the value), but it also seems quite remarkable at predicting the trends.

Column {data-width=600}
-----------------------------------------------------------------------

### Change in leading stocks

```{r}
bitcoin = read.csv("bitcoin.csv")
bitcoin$time = nrow(bitcoin):1
bitcoin$Date = as.numeric(gsub(".*/","",bitcoin$Date))
ggplot(bitcoin,aes(x = time,y = Close.Last,col = factor(Date)))+
  geom_line(aes(x=time,y=Close.Last),col="gray",size=3)+
  geom_smooth(size=3)+
  geom_line(aes(x=time,y=c(final_results$fitted,final_results$mean)))+
  theme(panel.grid.major = element_blank(), panel.grid.minor = element_blank(),
        panel.background = element_blank(), axis.line = element_line(colour = "black"),
        text = element_text(size=20),axis.ticks.x = element_blank(),
        legend.position = "None",axis.text.x = element_blank())+
  labs(y="Market Value \n(US Dollars)",x="time (2019-2024)",title="Bitcoin")+
  scale_color_viridis_d()
```